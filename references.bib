@article{grokking2022,
  title={Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{circuits2020,
  title={Zoom In: An Introduction to Circuits},
  author={Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  journal={Distill},
  year={2020},
  url={https://distill.pub/2020/circuits/zoom-in/}
}

@article{superposition2022,
  title={Toy Models of Superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  year={2022},
  url={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@article{induction2022,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and others},
  journal={Transformer Circuits Thread},
  year={2022},
  url={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@article{monosemanticity2023,
  title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author={Bricken, Trenton and others},
  journal={Transformer Circuits Thread},
  year={2023},
  url={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}

@article{mathframework2021,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  year={2021},
  url={https://transformer-circuits.pub/2021/framework/index.html}
}
