---
title: "First Principles of Mechanistic Interpretability"
---

# Welcome {.unnumbered}

This is a first-principles exploration of **mechanistic interpretability** — the project of reverse engineering neural networks to understand *how* they work, not just *that* they work.

## What This Series Is

A 15-article journey from foundations to practice:

- **Arc I: Foundations** (Articles 1-4) — What are we trying to understand? The transformer architecture, residual stream, and geometric structure of representations.
- **Arc II: Core Theory** (Articles 5-8) — Features, superposition, toy models, and circuits. The conceptual framework for interpretation.
- **Arc III: Techniques** (Articles 9-12) — Sparse autoencoders, attribution, activation patching, and ablation. The tools of the trade.
- **Arc IV: Synthesis** (Articles 13-15) — Induction heads as a complete case study, open problems in the field, and a practical guide to doing research.

## Who This Is For

- Software engineers curious about ML internals
- ML practitioners who want deeper understanding
- Performance engineers interested in AI
- Anyone who values understanding *why* over just *how*

## The Approach

We use **Polya's problem-solving framework** throughout: understand the problem before devising solutions, verify your understanding through intervention, and always ask "what would make this explanation wrong?"

We also bring a **performance engineering mindset**: you can't optimize what you don't understand, measure before you interpret, and never trust unvalidated claims.

## Getting Started

Start with [Article 1: Why Reverse Engineer Neural Networks?](articles/01-why-reverse-engineer.qmd) to understand the motivation and scope of the project.

## Also Published On

This series is also available on [Software Bits](https://softwarebits.substack.com/) on Substack.

---

*Last updated: January 2025*
