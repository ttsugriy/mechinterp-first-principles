{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Induction Heads - Hands-On Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttsugriy/mechinterp-first-principles/blob/main/notebooks/13-induction-heads.ipynb)\n",
    "\n",
    "This notebook accompanies [Chapter 13: Induction Heads](https://ttsugriy.github.io/mechinterp-first-principles/chapters/13-induction-heads.html).\n",
    "\n",
    "**What you'll do:**\n",
    "1. Find induction heads in GPT-2 Small\n",
    "2. Verify they perform the induction pattern\n",
    "3. Ablate them and measure the effect\n",
    "4. Understand the two-layer circuit\n",
    "\n",
    "**Time:** ~45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install libraries (run this cell, then restart runtime!)\n!pip install -q transformer-lens circuitsvis einops\n\nprint(\"Installation complete!\")\nprint(\"Now restart the runtime: Runtime -> Restart runtime\")\nprint(\"Then skip this cell and run the next one.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Import libraries (run this after restarting runtime)\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformer_lens import HookedTransformer, utils\nimport circuitsvis as cv\nfrom einops import rearrange\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "print(f\"Loaded GPT-2 Small: {model.cfg.n_layers} layers, {model.cfg.n_heads} heads per layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Induction Task\n",
    "\n",
    "Induction heads detect repeated patterns: if \"A B\" appeared before, and we see \"A\" again, predict \"B\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence with repetition\n",
    "# Format: [random tokens] A B [random tokens] A -> should predict B\n",
    "\n",
    "def create_induction_sequence(token_a=\"Paris\", token_b=\"France\"):\n",
    "    \"\"\"Create a sequence that tests induction.\"\"\"\n",
    "    text = f\"The city of {token_a} is in {token_b}. I visited {token_a}\"\n",
    "    return text\n",
    "\n",
    "text = create_induction_sequence()\n",
    "print(f\"Test sequence: '{text}'\")\n",
    "print(f\"Expected: model should predict 'France' or similar after second 'Paris'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    "tokens = model.to_tokens(text)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# What does it predict?\n",
    "final_logits = logits[0, -1]\n",
    "top_preds = torch.topk(final_logits, k=5)\n",
    "\n",
    "print(\"Top predictions after 'Paris':\")\n",
    "for idx, logit in zip(top_preds.indices, top_preds.values):\n",
    "    print(f\"  '{model.to_string(idx)}' (logit={logit:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find Induction Heads\n",
    "\n",
    "Induction heads have a distinctive attention pattern: they attend to the token that *followed* the previous occurrence of the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple repeated sequence for clearer visualization\n",
    "repeated_text = \"A B C D E A B C D\"\n",
    "repeated_tokens = model.to_tokens(repeated_text)\n",
    "_, repeated_cache = model.run_with_cache(repeated_tokens)\n",
    "\n",
    "token_strs = [model.to_string(t) for t in repeated_tokens[0]]\n",
    "print(f\"Tokens: {token_strs}\")\n",
    "print(f\"\\nIf induction works: position 6 ('A') should attend to position 1 ('B')\")\n",
    "print(f\"Because 'B' followed 'A' at positions 0-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_induction_score(cache, seq_len):\n",
    "    \"\"\"\n",
    "    Measure how much each head attends to the \"induction position\".\n",
    "    For repeated sequence A B C D E A B C D:\n",
    "    - Position 6 (second A) should attend to position 1 (first B)\n",
    "    - Position 7 (second B) should attend to position 2 (first C)\n",
    "    etc.\n",
    "    \"\"\"\n",
    "    scores = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        attn = cache[\"pattern\", layer][0]  # (heads, seq, seq)\n",
    "        \n",
    "        for head in range(model.cfg.n_heads):\n",
    "            # For the repeated part, check if it attends to induction position\n",
    "            # Position 6 should attend to position 1 (offset by 5, looking at +1)\n",
    "            induction_attn = 0\n",
    "            count = 0\n",
    "            \n",
    "            # Positions 6, 7, 8, 9 are the repeated A, B, C, D\n",
    "            for query_pos in range(6, min(10, seq_len)):\n",
    "                # The induction target is query_pos - 5 + 1 = query_pos - 4\n",
    "                key_pos = query_pos - 4\n",
    "                if key_pos > 0 and key_pos < query_pos:\n",
    "                    induction_attn += attn[head, query_pos, key_pos].item()\n",
    "                    count += 1\n",
    "            \n",
    "            if count > 0:\n",
    "                scores[layer, head] = induction_attn / count\n",
    "    \n",
    "    return scores\n",
    "\n",
    "induction_scores = measure_induction_score(repeated_cache, len(token_strs))\n",
    "\n",
    "# Find top induction heads\n",
    "flat_scores = induction_scores.flatten()\n",
    "top_indices = torch.topk(flat_scores, k=10).indices\n",
    "\n",
    "print(\"Top 10 heads by induction score:\")\n",
    "for idx in top_indices:\n",
    "    layer = idx.item() // model.cfg.n_heads\n",
    "    head = idx.item() % model.cfg.n_heads\n",
    "    score = induction_scores[layer, head].item()\n",
    "    print(f\"  Layer {layer}, Head {head}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the induction scores as a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(induction_scores.numpy(), cmap='Reds', aspect='auto')\n",
    "plt.colorbar(label='Induction Score')\n",
    "plt.xlabel('Head')\n",
    "plt.ylabel('Layer')\n",
    "plt.title('Induction Scores by Head\\n(Higher = more induction-like behavior)')\n",
    "plt.xticks(range(model.cfg.n_heads))\n",
    "plt.yticks(range(model.cfg.n_layers))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize an Induction Head\n",
    "\n",
    "Let's look at the attention pattern of a high-scoring induction head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best induction head\n",
    "best_idx = torch.argmax(induction_scores)\n",
    "best_layer = best_idx.item() // model.cfg.n_heads\n",
    "best_head = best_idx.item() % model.cfg.n_heads\n",
    "\n",
    "print(f\"Best induction head: Layer {best_layer}, Head {best_head}\")\n",
    "print(f\"Score: {induction_scores[best_layer, best_head]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize its attention pattern\n",
    "attn_pattern = repeated_cache[\"pattern\", best_layer][0, best_head].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attn_pattern, cmap='Blues')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks(range(len(token_strs)), token_strs, rotation=45, ha='right')\n",
    "plt.yticks(range(len(token_strs)), token_strs)\n",
    "plt.xlabel('Key (attending to)')\n",
    "plt.ylabel('Query (attending from)')\n",
    "plt.title(f'Induction Head Attention Pattern\\nLayer {best_layer}, Head {best_head}')\n",
    "\n",
    "# Draw boxes around induction positions\n",
    "for i, (q, k) in enumerate([(6, 2), (7, 3), (8, 4), (9, 5)]):\n",
    "    if q < len(token_strs) and k < len(token_strs):\n",
    "        plt.plot(k, q, 'ro', markersize=15, fillstyle='none', markeredgewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Red circles mark induction positions:\")\n",
    "print(\"Position 6 (A) → Position 2 (C): attend to what followed previous A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ablate the Induction Head\n",
    "\n",
    "If we remove the induction head, the model should perform worse on induction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_induction_loss(model, text, target_token):\n",
    "    \"\"\"Measure how well the model predicts the target after induction.\"\"\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    logits = model(tokens)\n",
    "    \n",
    "    # Get logit for target token at last position\n",
    "    target_id = model.to_tokens(target_token)[0, 1]  # Skip BOS\n",
    "    target_logit = logits[0, -1, target_id].item()\n",
    "    \n",
    "    # Get rank of target\n",
    "    rank = (logits[0, -1] > target_logit).sum().item()\n",
    "    \n",
    "    return target_logit, rank\n",
    "\n",
    "# Test on our induction sequence\n",
    "test_text = \"The word hello is followed by world. The word hello is followed by\"\n",
    "target = \" world\"\n",
    "\n",
    "clean_logit, clean_rank = get_induction_loss(model, test_text, target)\n",
    "print(f\"Clean model:\")\n",
    "print(f\"  Target '{target}' logit: {clean_logit:.2f}\")\n",
    "print(f\"  Target rank: {clean_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablate_head(layer, head):\n",
    "    \"\"\"Create a hook that zeros out a specific head.\"\"\"\n",
    "    def hook(activation, hook):\n",
    "        activation[:, :, head, :] = 0\n",
    "        return activation\n",
    "    return hook\n",
    "\n",
    "# Ablate the best induction head\n",
    "hook_name = f\"blocks.{best_layer}.attn.hook_z\"\n",
    "\n",
    "with model.hooks(fwd_hooks=[(hook_name, ablate_head(best_layer, best_head))]):\n",
    "    ablated_logit, ablated_rank = get_induction_loss(model, test_text, target)\n",
    "\n",
    "print(f\"\\nWith Layer {best_layer} Head {best_head} ablated:\")\n",
    "print(f\"  Target '{target}' logit: {ablated_logit:.2f} (was {clean_logit:.2f})\")\n",
    "print(f\"  Target rank: {ablated_rank} (was {clean_rank})\")\n",
    "print(f\"\\n  Logit drop: {clean_logit - ablated_logit:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablate all top induction heads\n",
    "top_heads = [(idx.item() // model.cfg.n_heads, idx.item() % model.cfg.n_heads) \n",
    "             for idx in torch.topk(flat_scores, k=5).indices]\n",
    "\n",
    "hooks = [(f\"blocks.{l}.attn.hook_z\", ablate_head(l, h)) for l, h in top_heads]\n",
    "\n",
    "with model.hooks(fwd_hooks=hooks):\n",
    "    multi_ablated_logit, multi_ablated_rank = get_induction_loss(model, test_text, target)\n",
    "\n",
    "print(f\"With top 5 induction heads ablated:\")\n",
    "print(f\"  Target logit: {multi_ablated_logit:.2f} (was {clean_logit:.2f})\")\n",
    "print(f\"  Target rank: {multi_ablated_rank} (was {clean_rank})\")\n",
    "print(f\"\\n  Total logit drop: {clean_logit - multi_ablated_logit:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Two-Layer Circuit\n",
    "\n",
    "Induction heads work via composition with \"previous token\" heads in earlier layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_prev_token_score(cache, seq_len):\n",
    "    \"\"\"\n",
    "    Measure how much each head attends to the previous token.\n",
    "    Previous token heads should have high attention on the diagonal-1.\n",
    "    \"\"\"\n",
    "    scores = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        attn = cache[\"pattern\", layer][0]  # (heads, seq, seq)\n",
    "        \n",
    "        for head in range(model.cfg.n_heads):\n",
    "            # Sum attention to previous token (diagonal - 1)\n",
    "            prev_attn = 0\n",
    "            for pos in range(1, seq_len):\n",
    "                prev_attn += attn[head, pos, pos-1].item()\n",
    "            scores[layer, head] = prev_attn / (seq_len - 1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "prev_token_scores = measure_prev_token_score(repeated_cache, len(token_strs))\n",
    "\n",
    "# Find top previous token heads\n",
    "flat_prev_scores = prev_token_scores.flatten()\n",
    "top_prev_indices = torch.topk(flat_prev_scores, k=5).indices\n",
    "\n",
    "print(\"Top 5 'previous token' heads:\")\n",
    "for idx in top_prev_indices:\n",
    "    layer = idx.item() // model.cfg.n_heads\n",
    "    head = idx.item() % model.cfg.n_heads\n",
    "    score = prev_token_scores[layer, head].item()\n",
    "    print(f\"  Layer {layer}, Head {head}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: previous token heads are in early layers, induction heads in later layers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im1 = axes[0].imshow(prev_token_scores.numpy(), cmap='Blues', aspect='auto')\n",
    "axes[0].set_xlabel('Head')\n",
    "axes[0].set_ylabel('Layer')\n",
    "axes[0].set_title('Previous Token Scores\\n(Early layers)')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(induction_scores.numpy(), cmap='Reds', aspect='auto')\n",
    "axes[1].set_xlabel('Head')\n",
    "axes[1].set_ylabel('Layer')\n",
    "axes[1].set_title('Induction Scores\\n(Later layers)')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Previous token heads cluster in early layers (0-2)\")\n",
    "print(\"        Induction heads cluster in later layers (5+)\")\n",
    "print(\"This is the two-layer circuit: early heads enable later heads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Test on different patterns\n",
    "Try longer repeated sequences. Does the induction pattern still work?\n",
    "\n",
    "### Exercise 2: Ablate previous token heads\n",
    "What happens if you ablate the previous token heads instead of the induction heads?\n",
    "\n",
    "### Exercise 3: Find the composition\n",
    "Can you measure how the induction head queries are influenced by the previous token head outputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "longer_text = \"X Y Z W X Y Z W X Y Z W X Y\"\n",
    "# Does induction still work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now:\n",
    "1. Found induction heads in GPT-2 using attention pattern analysis\n",
    "2. Visualized the distinctive \"stripe\" pattern of induction attention\n",
    "3. Verified causality through ablation\n",
    "4. Identified the two-layer circuit: previous token heads → induction heads\n",
    "\n",
    "This is the complete mechanistic interpretability workflow: \n",
    "**observe → hypothesize → intervene → verify**\n",
    "\n",
    "**Next:** [Chapter 14: Open Problems](https://ttsugriy.github.io/mechinterp-first-principles/chapters/14-open-problems.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}