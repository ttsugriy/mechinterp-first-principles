{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Sparse Autoencoders - Hands-On Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttsugriy/mechinterp-first-principles/blob/main/notebooks/09-sparse-autoencoders.ipynb)\n",
    "\n",
    "This notebook accompanies [Chapter 9: Sparse Autoencoders](https://ttsugriy.github.io/mechinterp-first-principles/chapters/09-sparse-autoencoders.html) from *First Principles of Mechanistic Interpretability*.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Load a pre-trained SAE for GPT-2 Small\n",
    "2. Extract features from model activations\n",
    "3. Explore what features represent\n",
    "4. Try steering the model with a feature\n",
    "\n",
    "**Time:** ~30 minutes\n",
    "\n",
    "**Prerequisites:** Basic Python, having read Chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required libraries. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Install libraries (run this cell, then restart runtime!)\n!pip install -q transformer-lens sae-lens einops jaxtyping\n\nprint(\"Installation complete!\")\nprint(\"Now restart the runtime: Runtime -> Restart runtime\")\nprint(\"Then skip this cell and run the next one.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Import libraries (run this after restarting runtime)\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformer_lens import HookedTransformer\nfrom sae_lens import SAE\n\n# Check for GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\n# For reproducibility\ntorch.manual_seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model and SAE\n",
    "\n",
    "We'll use GPT-2 Small and a pre-trained SAE from the SAE Lens library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 Small\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "print(f\"Model loaded: {model.cfg.n_layers} layers, {model.cfg.d_model} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained SAE for layer 8 residual stream\n",
    "# This SAE was trained by Joseph Bloom and is available through SAE Lens\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.8.hook_resid_pre\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"SAE loaded!\")\n",
    "print(f\"  Input dimension: {sae.cfg.d_in}\")\n",
    "print(f\"  Number of features: {sae.cfg.d_sae}\")\n",
    "print(f\"  Expansion factor: {sae.cfg.d_sae / sae.cfg.d_in:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features from Text\n",
    "\n",
    "Let's see what features activate for a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"The Golden Gate Bridge is a famous landmark in San Francisco.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = model.to_tokens(text)\n",
    "print(f\"Tokens: {[model.to_string(t) for t in tokens[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model and get activations at layer 8\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "activations = cache[\"blocks.8.hook_resid_pre\"]\n",
    "\n",
    "print(f\"Activation shape: {activations.shape}\")\n",
    "print(f\"  (batch, sequence_length, d_model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass activations through the SAE to get feature activations\n",
    "feature_acts = sae.encode(activations)\n",
    "\n",
    "print(f\"Feature activation shape: {feature_acts.shape}\")\n",
    "print(f\"  (batch, sequence_length, n_features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How sparse are the activations?\n",
    "nonzero_per_position = (feature_acts[0] > 0).sum(dim=-1).float()\n",
    "total_features = feature_acts.shape[-1]\n",
    "\n",
    "print(f\"\\nSparsity analysis:\")\n",
    "print(f\"  Total features: {total_features}\")\n",
    "print(f\"  Avg active per position: {nonzero_per_position.mean():.1f}\")\n",
    "print(f\"  Sparsity: {100 * (1 - nonzero_per_position.mean() / total_features):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Individual Features\n",
    "\n",
    "Let's look at which features activate most strongly for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token position, find the top activating features\n",
    "token_strs = [model.to_string(t) for t in tokens[0]]\n",
    "\n",
    "print(\"Top 3 features per token:\\n\")\n",
    "for pos, token_str in enumerate(token_strs):\n",
    "    acts = feature_acts[0, pos]\n",
    "    top_features = torch.topk(acts, k=3)\n",
    "    \n",
    "    feature_info = [f\"#{idx.item()}({val:.2f})\" \n",
    "                    for idx, val in zip(top_features.indices, top_features.values)]\n",
    "    print(f\"{token_str:15} → {', '.join(feature_info)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the single most active feature across all positions\n",
    "max_act = feature_acts[0].max()\n",
    "max_pos = (feature_acts[0] == max_act).nonzero()[0]\n",
    "max_feature_idx = max_pos[1].item()\n",
    "max_token_pos = max_pos[0].item()\n",
    "\n",
    "print(f\"Most active feature: #{max_feature_idx}\")\n",
    "print(f\"  Activation: {max_act:.2f}\")\n",
    "print(f\"  Token: '{token_strs[max_token_pos]}' (position {max_token_pos})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Investigate a Specific Feature\n",
    "\n",
    "Let's test what a specific feature responds to by running it on multiple texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_activation(text, feature_idx):\n",
    "    \"\"\"Get the max activation of a feature for a given text.\"\"\"\n",
    "    tokens = model.to_tokens(text)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    activations = cache[\"blocks.8.hook_resid_pre\"]\n",
    "    feature_acts = sae.encode(activations)\n",
    "    return feature_acts[0, :, feature_idx].max().item()\n",
    "\n",
    "# Test the most active feature on related and unrelated texts\n",
    "feature_to_test = max_feature_idx\n",
    "\n",
    "test_texts = [\n",
    "    \"The Golden Gate Bridge is beautiful.\",\n",
    "    \"San Francisco is a city in California.\",\n",
    "    \"The bridge spans the bay.\",\n",
    "    \"I love eating pizza.\",\n",
    "    \"Python is a programming language.\",\n",
    "    \"The weather is nice today.\",\n",
    "]\n",
    "\n",
    "print(f\"Testing feature #{feature_to_test}:\\n\")\n",
    "for text in test_texts:\n",
    "    act = get_feature_activation(text, feature_to_test)\n",
    "    bar = \"█\" * int(act * 5)\n",
    "    print(f\"{act:5.2f} {bar:20} {text[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reconstruction Quality\n",
    "\n",
    "SAEs trade off reconstruction accuracy for sparsity. Let's measure this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original activations\n",
    "text = \"The capital of France is Paris.\"\n",
    "tokens = model.to_tokens(text)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "original_acts = cache[\"blocks.8.hook_resid_pre\"]\n",
    "\n",
    "# Encode then decode through SAE\n",
    "feature_acts = sae.encode(original_acts)\n",
    "reconstructed_acts = sae.decode(feature_acts)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = ((original_acts - reconstructed_acts) ** 2).mean().item()\n",
    "original_variance = original_acts.var().item()\n",
    "explained_variance = 1 - mse / original_variance\n",
    "\n",
    "print(f\"Reconstruction quality:\")\n",
    "print(f\"  MSE: {mse:.4f}\")\n",
    "print(f\"  Explained variance: {explained_variance:.1%}\")\n",
    "print(f\"  (Higher is better, typically 85-95% for good SAEs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Steering with Features (Advanced)\n",
    "\n",
    "The famous \"Golden Gate Bridge\" experiment showed you can steer model behavior by amplifying features. Let's try a simpler version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_steering(prompt, feature_idx, steering_strength=0.0, max_tokens=20):\n",
    "    \"\"\"Generate text with optional feature steering.\"\"\"\n",
    "    \n",
    "    # Get the feature direction\n",
    "    feature_direction = sae.W_dec[feature_idx]  # Shape: (d_model,)\n",
    "    \n",
    "    def steering_hook(activation, hook):\n",
    "        # Add the feature direction scaled by steering strength\n",
    "        activation[:, :, :] += steering_strength * feature_direction\n",
    "        return activation\n",
    "    \n",
    "    # Generate with hook\n",
    "    if steering_strength != 0:\n",
    "        with model.hooks(fwd_hooks=[(\"blocks.8.hook_resid_pre\", steering_hook)]):\n",
    "            output = model.generate(\n",
    "                prompt, \n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True\n",
    "            )\n",
    "    else:\n",
    "        output = model.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=max_tokens, \n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    return model.to_string(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a feature that might be interesting to steer with\n",
    "# Let's look for a feature that activates on a specific concept\n",
    "\n",
    "prompt = \"I went to the store to buy\"\n",
    "\n",
    "print(\"Generation WITHOUT steering:\")\n",
    "for i in range(3):\n",
    "    result = generate_with_steering(prompt, feature_idx=0, steering_strength=0.0)\n",
    "    print(f\"  {result}\")\n",
    "\n",
    "print(f\"\\nGeneration WITH steering (feature #{max_feature_idx}, strength=5.0):\")\n",
    "for i in range(3):\n",
    "    result = generate_with_steering(prompt, feature_idx=max_feature_idx, steering_strength=5.0)\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore on Neuronpedia\n",
    "\n",
    "For any feature you find interesting, you can look it up on Neuronpedia to see:\n",
    "- Max activating examples\n",
    "- Feature descriptions\n",
    "- Related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuronpedia_url(feature_idx, layer=8):\n",
    "    \"\"\"Generate a Neuronpedia URL for a feature.\"\"\"\n",
    "    return f\"https://www.neuronpedia.org/gpt2-small/{layer}-res-jb/{feature_idx}\"\n",
    "\n",
    "print(f\"Explore feature #{max_feature_idx} on Neuronpedia:\")\n",
    "print(neuronpedia_url(max_feature_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Find a \"code\" feature\n",
    "Run the SAE on programming-related text and find a feature that activates strongly for code concepts.\n",
    "\n",
    "### Exercise 2: Compare layers\n",
    "Load SAEs for different layers (e.g., layer 0 vs layer 10). How do the features differ?\n",
    "\n",
    "### Exercise 3: Measure sparsity vs reconstruction trade-off\n",
    "Try different texts and plot the relationship between number of active features and reconstruction quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "code_text = \"def hello_world():\\n    print('Hello, world!')\"\n",
    "\n",
    "# Find top features for this text\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you:\n",
    "1. Loaded a pre-trained SAE for GPT-2 Small\n",
    "2. Extracted sparse feature activations from text\n",
    "3. Explored what individual features respond to\n",
    "4. Measured reconstruction quality\n",
    "5. Experimented with feature steering\n",
    "\n",
    "**Next steps:**\n",
    "- Explore more features on [Neuronpedia](https://www.neuronpedia.org/gpt2-small)\n",
    "- Try training your own SAE with [SAE Lens](https://github.com/jbloomAus/SAELens)\n",
    "- Read [Chapter 10: Attribution](https://ttsugriy.github.io/mechinterp-first-principles/chapters/10-attribution.html) to learn how to trace feature contributions to outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}