{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Mechanistic Interpretability Exercises\n",
    "\n",
    "Practice problems from [First Principles of Mechanistic Interpretability](https://ttsugriy.github.io/mechinterp-first-principles/).\n",
    "\n",
    "**Instructions:**\n",
    "1. Run the setup cell first\n",
    "2. Try each exercise before looking at the solution\n",
    "3. Solutions are in collapsed cells - click to expand\n",
    "\n",
    "**Tip:** Make sure you have a GPU runtime enabled (Runtime → Change runtime type → T4 GPU)"
   ],
   "metadata": {
    "id": "header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "id": "setup-header"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": "# Step 1: Install libraries (run this cell, then restart runtime!)\n# After running, go to Runtime → Restart runtime, then skip to the next cell\n\n!pip install -q transformer-lens einops jaxtyping circuitsvis plotly sae-lens\n\nprint(\"✅ Installation complete!\")\nprint(\"⚠️  Now restart the runtime: Runtime → Restart runtime\")\nprint(\"   Then skip this cell and run the next one.\")"
  },
  {
   "cell_type": "code",
   "source": "# Step 2: Import libraries and load model (run this after restarting runtime)\n\nimport torch\nimport transformer_lens as tl\nfrom transformer_lens import utils\nimport plotly.express as px\nimport pandas as pd\n\n# Check GPU\nprint(f\"GPU available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# Load model (used throughout)\nmodel = tl.HookedTransformer.from_pretrained(\"gpt2-small\")\nprint(f\"\\nLoaded {model.cfg.model_name}: {model.cfg.n_layers} layers, {model.cfg.n_heads} heads\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Arc I: Foundations\n",
    "\n",
    "### Chapter 2: Transformers"
   ],
   "metadata": {
    "id": "arc1-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2.1: Attention Pattern Analysis\n",
    "\n",
    "**Problem**: Load GPT-2 Small and run the prompt \"The cat sat on the\". Visualize the attention pattern for layer 0, head 0. What token does \"the\" (final position) attend to most?\n",
    "\n",
    "**Hint**: Use `model.run_with_cache()` and access `cache[\"pattern\", 0][0, 0]` for layer 0, head 0."
   ],
   "metadata": {
    "id": "ex-2-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Tokenize the prompt\n",
    "# 2. Run with cache\n",
    "# 3. Extract attention pattern for layer 0, head 0\n",
    "# 4. Print what the final token attends to\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-2-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 2.1 (click to expand)\n",
    "tokens = model.to_tokens(\"The cat sat on the\")\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# Layer 0, head 0 attention pattern\n",
    "pattern = cache[\"pattern\", 0][0, 0]  # [seq_len, seq_len]\n",
    "\n",
    "# What does final position attend to?\n",
    "final_attn = pattern[-1]\n",
    "print(\"Attention from final 'the':\")\n",
    "for i, (tok, attn) in enumerate(zip(model.to_str_tokens(tokens[0]), final_attn)):\n",
    "    print(f\"  {tok}: {attn:.3f}\")\n",
    "\n",
    "# Typically: early heads attend to nearby tokens or BOS"
   ],
   "metadata": {
    "id": "ex-2-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 2.2: MLP Activation Sparsity\n",
    "\n",
    "**Problem**: For the same prompt, compute the fraction of MLP neurons that are active (> 0 after GELU) in layer 5. Is MLP activation sparse?\n",
    "\n",
    "**Hint**: Access `cache[\"post\", 5]` for post-GELU activations. Count how many are > 0."
   ],
   "metadata": {
    "id": "ex-2-2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-2-2-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 2.2 (click to expand)\n",
    "mlp_post = cache[\"post\", 5][0]  # [seq_len, d_mlp]\n",
    "active = (mlp_post > 0).float().mean()\n",
    "print(f\"Fraction active: {active:.1%}\")\n",
    "# Typically 30-50% are active - moderately sparse"
   ],
   "metadata": {
    "id": "ex-2-2-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapter 3: Residual Stream"
   ],
   "metadata": {
    "id": "ch3-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3.1: Logit Lens\n",
    "\n",
    "**Problem**: Run \"The Eiffel Tower is in\" through GPT-2 Small. At each layer, project the residual stream to vocabulary space and find the top prediction. At which layer does \"Paris\" first appear in top 5?"
   ],
   "metadata": {
    "id": "ex-3-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-3-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 3.1 (click to expand)\n",
    "prompt = \"The Eiffel Tower is in\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "\n",
    "for layer in range(12):\n",
    "    resid = cache[\"resid_post\", layer][0, -1]\n",
    "    logits = resid @ model.W_U\n",
    "    top5 = logits.topk(5).indices\n",
    "    top5_tokens = [model.tokenizer.decode(t) for t in top5]\n",
    "    has_paris = \" Paris\" in top5_tokens or \"Paris\" in top5_tokens\n",
    "    print(f\"Layer {layer}: {top5_tokens} {'<-- Paris!' if has_paris else ''}\")"
   ],
   "metadata": {
    "id": "ex-3-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3.2: Component Contribution\n",
    "\n",
    "**Problem**: For the same prompt, which single component (attention head or MLP) contributes most to the \"Paris\" logit?"
   ],
   "metadata": {
    "id": "ex-3-2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-3-2-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# @title Solution 3.2 (click to expand)\nparis_token = model.to_single_token(\" Paris\")\nparis_dir = model.W_U[:, paris_token]\n\ncontributions = []\nfor layer in range(12):\n    # MLP contribution\n    mlp_out = cache[\"mlp_out\", layer][0, -1]\n    contributions.append((f\"L{layer}_MLP\", (mlp_out @ paris_dir).item()))\n\n    # Each attention head's contribution (compute from z and W_O)\n    z = cache[\"z\", layer][0, -1]  # [n_heads, d_head]\n    W_O = model.W_O[layer]  # [n_heads, d_head, d_model]\n    for head in range(12):\n        head_out = z[head] @ W_O[head]  # [d_model]\n        contributions.append((f\"L{layer}H{head}\", (head_out @ paris_dir).item()))\n\ntop = sorted(contributions, key=lambda x: -x[1])[:5]\nprint(\"Top contributors to 'Paris':\")\nfor name, val in top:\n    print(f\"  {name}: {val:+.2f}\")",
   "metadata": {
    "id": "ex-3-2-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapter 4: Geometry"
   ],
   "metadata": {
    "id": "ch4-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 4.1: Semantic Clustering\n",
    "\n",
    "**Problem**: Get the unembedding vectors for: \"Paris\", \"London\", \"Berlin\", \"cat\", \"dog\", \"fish\". Compute pairwise cosine similarities. Do cities cluster together? Do animals?"
   ],
   "metadata": {
    "id": "ex-4-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-4-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 4.1 (click to expand)\n",
    "words = [\" Paris\", \" London\", \" Berlin\", \" cat\", \" dog\", \" fish\"]\n",
    "tokens = [model.to_single_token(w) for w in words]\n",
    "vecs = torch.stack([model.W_U[:, t] for t in tokens])\n",
    "\n",
    "# Pairwise cosine similarity\n",
    "vecs_norm = vecs / vecs.norm(dim=1, keepdim=True)\n",
    "sims = vecs_norm @ vecs_norm.T\n",
    "\n",
    "print(\"Cosine similarities:\")\n",
    "for i, w1 in enumerate(words):\n",
    "    for j, w2 in enumerate(words):\n",
    "        if j > i:\n",
    "            print(f\"  {w1} - {w2}: {sims[i,j]:.3f}\")\n",
    "\n",
    "# Cities should have higher similarity with each other than with animals"
   ],
   "metadata": {
    "id": "ex-4-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Arc II: Core Theory\n",
    "\n",
    "### Chapter 5: Features"
   ],
   "metadata": {
    "id": "arc2-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 5.1: Feature Direction Extraction\n",
    "\n",
    "**Problem**: Create a \"sentiment\" direction by taking the difference between embeddings of positive and negative words. Test if this direction correlates with sentiment in new sentences."
   ],
   "metadata": {
    "id": "ex-5-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-5-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 5.1 (click to expand)\n",
    "positive = [\" good\", \" great\", \" excellent\", \" wonderful\"]\n",
    "negative = [\" bad\", \" terrible\", \" awful\", \" horrible\"]\n",
    "\n",
    "pos_vecs = torch.stack([model.W_E[model.to_single_token(w)] for w in positive])\n",
    "neg_vecs = torch.stack([model.W_E[model.to_single_token(w)] for w in negative])\n",
    "\n",
    "sentiment_dir = pos_vecs.mean(0) - neg_vecs.mean(0)\n",
    "sentiment_dir = sentiment_dir / sentiment_dir.norm()\n",
    "\n",
    "# Test on new sentences\n",
    "test = [\"This movie is fantastic\", \"This movie is terrible\"]\n",
    "for sent in test:\n",
    "    tokens = model.to_tokens(sent)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    final = cache[\"resid_post\", 11][0, -1]\n",
    "    score = (final @ sentiment_dir).item()\n",
    "    print(f\"{sent}: {score:+.2f}\")"
   ],
   "metadata": {
    "id": "ex-5-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapter 6: Superposition"
   ],
   "metadata": {
    "id": "ch6-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 6.1: Measuring Interference\n",
    "\n",
    "**Problem**: The residual stream has 768 dimensions. If features were orthogonal, we could store at most 768. But models represent many more concepts. Pick 100 random unembedding vectors and compute the average absolute cosine similarity. How close to orthogonal are they?"
   ],
   "metadata": {
    "id": "ex-6-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-6-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 6.1 (click to expand)\n",
    "import random\n",
    "\n",
    "# Sample 100 random tokens\n",
    "all_tokens = list(range(model.cfg.d_vocab))\n",
    "sample = random.sample(all_tokens, 100)\n",
    "vecs = model.W_U[:, sample].T  # [100, 768]\n",
    "vecs = vecs / vecs.norm(dim=1, keepdim=True)\n",
    "\n",
    "# Average absolute cosine similarity\n",
    "sims = (vecs @ vecs.T).abs()\n",
    "# Exclude diagonal\n",
    "mask = ~torch.eye(100, dtype=bool, device=sims.device)\n",
    "avg_sim = sims[mask].mean()\n",
    "\n",
    "print(f\"Average |cosine similarity|: {avg_sim:.4f}\")\n",
    "# Expected: ~0.02-0.05 (nearly orthogonal in high dimensions)"
   ],
   "metadata": {
    "id": "ex-6-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Arc III: Techniques\n",
    "\n",
    "### Chapter 9: SAEs"
   ],
   "metadata": {
    "id": "arc3-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 9.1: Feature Activation Analysis\n",
    "\n",
    "**Problem**: Load an SAE for GPT-2 Small layer 8. Run \"The president of the United States\" and find the top 5 most active features at the final position. Look them up on Neuronpedia."
   ],
   "metadata": {
    "id": "ex-9-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-9-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 9.1 (click to expand)\n",
    "from sae_lens import SAE\n",
    "\n",
    "sae, _, _ = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.8.hook_resid_pre\"\n",
    ")\n",
    "\n",
    "prompt = \"The president of the United States\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "\n",
    "resid = cache[\"resid_pre\", 8][0, -1]\n",
    "acts = sae.encode(resid.unsqueeze(0))[0]\n",
    "\n",
    "top5 = acts.topk(5)\n",
    "print(\"Top 5 features:\")\n",
    "for idx, val in zip(top5.indices, top5.values):\n",
    "    print(f\"  Feature {idx.item()}: {val.item():.2f}\")\n",
    "    print(f\"  https://neuronpedia.org/gpt2-small/8-res-jb/{idx.item()}\")"
   ],
   "metadata": {
    "id": "ex-9-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapter 10: Attribution"
   ],
   "metadata": {
    "id": "ch10-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 10.1: Full Attribution Decomposition\n",
    "\n",
    "**Problem**: For \"2 + 2 =\", decompose the logit for \" 4\" into contributions from each component. What fraction comes from MLPs vs attention?"
   ],
   "metadata": {
    "id": "ex-10-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-10-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 10.1 (click to expand)\n",
    "prompt = \"2 + 2 =\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "\n",
    "target = model.to_single_token(\" 4\")\n",
    "target_dir = model.W_U[:, target]\n",
    "\n",
    "mlp_total = 0\n",
    "attn_total = 0\n",
    "\n",
    "for layer in range(12):\n",
    "    mlp = cache[\"mlp_out\", layer][0, -1]\n",
    "    mlp_total += (mlp @ target_dir).item()\n",
    "\n",
    "    attn = cache[\"attn_out\", layer][0, -1]\n",
    "    attn_total += (attn @ target_dir).item()\n",
    "\n",
    "print(f\"MLP contribution: {mlp_total:.2f}\")\n",
    "print(f\"Attention contribution: {attn_total:.2f}\")\n",
    "print(f\"MLP fraction: {mlp_total / (mlp_total + attn_total):.1%}\")"
   ],
   "metadata": {
    "id": "ex-10-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapter 11: Patching"
   ],
   "metadata": {
    "id": "ch11-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 11.1: Activation Patching\n",
    "\n",
    "**Problem**: Patch the residual stream at layer 6 from \"The Louvre is in\" into \"The Colosseum is in\". Does the prediction change from \"Rome\" to \"Paris\"?"
   ],
   "metadata": {
    "id": "ex-11-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-11-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 11.1 (click to expand)\n",
    "clean = \"The Colosseum is in\"\n",
    "corrupt = \"The Louvre is in\"\n",
    "\n",
    "clean_tokens = model.to_tokens(clean)\n",
    "corrupt_tokens = model.to_tokens(corrupt)\n",
    "\n",
    "_, corrupt_cache = model.run_with_cache(corrupt_tokens)\n",
    "corrupt_resid = corrupt_cache[\"resid_post\", 6]\n",
    "\n",
    "def patch_hook(act, hook):\n",
    "    act[:, -1, :] = corrupt_resid[:, -1, :]\n",
    "    return act\n",
    "\n",
    "patched = model.run_with_hooks(\n",
    "    clean_tokens,\n",
    "    fwd_hooks=[(\"blocks.6.hook_resid_post\", patch_hook)]\n",
    ")\n",
    "\n",
    "rome = model.to_single_token(\" Rome\")\n",
    "paris = model.to_single_token(\" Paris\")\n",
    "\n",
    "print(f\"Rome logit: {patched[0, -1, rome]:.2f}\")\n",
    "print(f\"Paris logit: {patched[0, -1, paris]:.2f}\")\n",
    "print(f\"Prediction: {model.tokenizer.decode(patched[0, -1].argmax())}\")"
   ],
   "metadata": {
    "id": "ex-11-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chapter 12: Ablation"
   ],
   "metadata": {
    "id": "ch12-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 12.1: Attention Head Ablation\n",
    "\n",
    "**Problem**: Zero-ablate each attention head individually for \"The Eiffel Tower is in\". Which head, when ablated, most reduces the \"Paris\" logit?"
   ],
   "metadata": {
    "id": "ex-12-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "ex-12-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Solution 12.1 (click to expand)\n",
    "prompt = \"The Eiffel Tower is in\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "paris = model.to_single_token(\" Paris\")\n",
    "\n",
    "baseline = model(tokens)[0, -1, paris].item()\n",
    "print(f\"Baseline Paris logit: {baseline:.2f}\")\n",
    "\n",
    "results = []\n",
    "for layer in range(12):\n",
    "    for head in range(12):\n",
    "        def ablate_head(act, hook, h=head):\n",
    "            act[:, :, h, :] = 0\n",
    "            return act\n",
    "\n",
    "        ablated = model.run_with_hooks(\n",
    "            tokens,\n",
    "            fwd_hooks=[(f\"blocks.{layer}.hook_z\", ablate_head)]\n",
    "        )\n",
    "        new_logit = ablated[0, -1, paris].item()\n",
    "        results.append((f\"L{layer}H{head}\", baseline - new_logit))\n",
    "\n",
    "top = sorted(results, key=lambda x: -x[1])[:5]\n",
    "print(\"\\nHeads whose ablation most reduces Paris logit:\")\n",
    "for name, drop in top:\n",
    "    print(f\"  {name}: -{drop:.2f}\")"
   ],
   "metadata": {
    "id": "ex-12-1-solution",
    "cellView": "form"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Challenge Problems\n",
    "\n",
    "These are more open-ended and may take longer."
   ],
   "metadata": {
    "id": "challenges-header"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Challenge 1: Find a Factual Recall Circuit\n",
    "\n",
    "**Problem**: Analyze \"The capital of France is\" → \"Paris\". Identify:\n",
    "1. Which attention heads move information from \"France\" to the final position\n",
    "2. Which MLP layers retrieve the answer\n",
    "3. Verify with patching that these components are causally important\n",
    "\n",
    "This is a multi-hour exercise. Document your methodology and findings."
   ],
   "metadata": {
    "id": "challenge-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR EXPLORATION HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "challenge-1-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Challenge 2: Compare Arithmetic Circuits\n",
    "\n",
    "**Problem**: Compare the circuits for \"2 + 3 =\" vs \"7 + 8 =\".\n",
    "- Do they use the same components?\n",
    "- Is there evidence of different strategies for small vs large numbers?\n",
    "- What happens for \"99 + 1 =\"?"
   ],
   "metadata": {
    "id": "challenge-2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR EXPLORATION HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "challenge-2-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Challenge 3: Adversarial Feature Search\n",
    "\n",
    "**Problem**: Find an input that maximally activates a specific SAE feature. Start with feature 1000 in the layer 8 SAE. Use gradient-based optimization or greedy token search."
   ],
   "metadata": {
    "id": "challenge-3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR EXPLORATION HERE\n",
    "\n"
   ],
   "metadata": {
    "id": "challenge-3-work"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Completed all exercises? Here's what to do next:\n",
    "\n",
    "1. **[Running Example](https://ttsugriy.github.io/mechinterp-first-principles/running-example.html)** — See all techniques applied to one behavior\n",
    "2. **[Zoo of Circuits](https://ttsugriy.github.io/mechinterp-first-principles/zoo-of-circuits.html)** — Catalog of known circuits to study\n",
    "3. **[Open Problems](https://ttsugriy.github.io/mechinterp-first-principles/chapters/14-open-problems.html)** — Unsolved questions in the field\n",
    "4. **Start your own analysis!** Pick a behavior and investigate it."
   ],
   "metadata": {
    "id": "next-steps"
   }
  }
 ]
}