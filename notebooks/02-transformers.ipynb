{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Transformers - Hands-On Notebook\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttsugriy/mechinterp-first-principles/blob/main/notebooks/02-transformers.ipynb)\n",
    "\n",
    "This notebook accompanies [Chapter 2: Transformers as Matrix Multiplication Machines](https://ttsugriy.github.io/mechinterp-first-principles/chapters/02-transformers.html).\n",
    "\n",
    "**What you'll do:**\n",
    "1. Load GPT-2 and run a forward pass\n",
    "2. Inspect attention patterns\n",
    "3. Visualize how information flows\n",
    "4. Understand the residual stream\n",
    "\n",
    "**Time:** ~20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformer-lens circuitsvis -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformer_lens import HookedTransformer\n",
    "import circuitsvis as cv\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "print(f\"Model: GPT-2 Small\")\n",
    "print(f\"  Layers: {model.cfg.n_layers}\")\n",
    "print(f\"  Attention heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"  Model dimension (d_model): {model.cfg.d_model}\")\n",
    "print(f\"  Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run a Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test prompt\n",
    "text = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = model.to_tokens(text)\n",
    "print(f\"Input: '{text}'\")\n",
    "print(f\"Tokens: {tokens.shape} = {[model.to_string(t) for t in tokens[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model and cache all intermediate activations\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"  (batch_size, sequence_length, vocabulary_size)\")\n",
    "print(f\"\\nCached {len(cache)} activation tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the model predict?\n",
    "final_logits = logits[0, -1]  # Last token position\n",
    "top_tokens = torch.topk(final_logits, k=10)\n",
    "\n",
    "print(\"Top 10 predictions:\")\n",
    "for i, (idx, logit) in enumerate(zip(top_tokens.indices, top_tokens.values)):\n",
    "    prob = torch.softmax(final_logits, dim=-1)[idx]\n",
    "    print(f\"  {i+1}. '{model.to_string(idx)}' (logit={logit:.2f}, prob={prob:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Attention Patterns\n",
    "\n",
    "Attention patterns show which tokens attend to which other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention patterns from layer 5\n",
    "attention_pattern = cache[\"pattern\", 5]  # Shape: (batch, heads, query_pos, key_pos)\n",
    "print(f\"Attention pattern shape: {attention_pattern.shape}\")\n",
    "print(f\"  (batch, n_heads, seq_len, seq_len)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for all heads in layer 5\n",
    "token_strs = [model.to_string(t) for t in tokens[0]]\n",
    "\n",
    "cv.attention.attention_patterns(\n",
    "    tokens=token_strs,\n",
    "    attention=attention_pattern[0]  # Remove batch dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single head's attention pattern\n",
    "head_idx = 0\n",
    "layer_idx = 5\n",
    "\n",
    "attn = cache[\"pattern\", layer_idx][0, head_idx].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn, cmap='Blues')\n",
    "plt.xticks(range(len(token_strs)), token_strs, rotation=45, ha='right')\n",
    "plt.yticks(range(len(token_strs)), token_strs)\n",
    "plt.xlabel('Key (attending to)')\n",
    "plt.ylabel('Query (attending from)')\n",
    "plt.title(f'Attention Pattern: Layer {layer_idx}, Head {head_idx}')\n",
    "plt.colorbar(label='Attention weight')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Residual Stream\n",
    "\n",
    "The residual stream accumulates contributions from all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get residual stream at different layers\n",
    "resid_0 = cache[\"resid_pre\", 0]   # After embedding, before layer 0\n",
    "resid_6 = cache[\"resid_pre\", 6]   # After 6 layers\n",
    "resid_11 = cache[\"resid_post\", 11] # After final layer\n",
    "\n",
    "print(\"Residual stream shapes (all same):\")\n",
    "print(f\"  After embedding: {resid_0.shape}\")\n",
    "print(f\"  After layer 6: {resid_6.shape}\")\n",
    "print(f\"  After layer 11: {resid_11.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much does the residual stream change through the network?\n",
    "# Measure cosine similarity between early and late representations\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    return torch.nn.functional.cosine_similarity(a, b, dim=-1)\n",
    "\n",
    "# For the last token position\n",
    "sim_0_6 = cosine_sim(resid_0[0, -1], resid_6[0, -1]).item()\n",
    "sim_6_11 = cosine_sim(resid_6[0, -1], resid_11[0, -1]).item()\n",
    "sim_0_11 = cosine_sim(resid_0[0, -1], resid_11[0, -1]).item()\n",
    "\n",
    "print(f\"Cosine similarity of residual stream (last position):\")\n",
    "print(f\"  Layer 0 ↔ Layer 6:  {sim_0_6:.3f}\")\n",
    "print(f\"  Layer 6 ↔ Layer 11: {sim_6_11:.3f}\")\n",
    "print(f\"  Layer 0 ↔ Layer 11: {sim_0_11:.3f}\")\n",
    "print(f\"\\nThe residual stream changes significantly through the network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residual stream norm through layers\n",
    "norms = []\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    resid = cache[\"resid_pre\", layer][0, -1]  # Last token\n",
    "    norms.append(resid.norm().item())\n",
    "\n",
    "# Add final layer\n",
    "norms.append(cache[\"resid_post\", model.cfg.n_layers - 1][0, -1].norm().item())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(norms, 'o-')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Residual Stream Norm')\n",
    "plt.title('How the Residual Stream Grows Through the Network')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Component Contributions\n",
    "\n",
    "Each attention head and MLP adds to the residual stream. Let's see their contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output of each attention layer and MLP\n",
    "attn_out = cache[\"attn_out\", 5][0, -1]  # Layer 5 attention output, last position\n",
    "mlp_out = cache[\"mlp_out\", 5][0, -1]    # Layer 5 MLP output, last position\n",
    "\n",
    "print(f\"Attention output norm: {attn_out.norm():.2f}\")\n",
    "print(f\"MLP output norm: {mlp_out.norm():.2f}\")\n",
    "print(f\"\\nMLP typically has larger contributions than attention!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all layers\n",
    "attn_norms = [cache[\"attn_out\", l][0, -1].norm().item() for l in range(model.cfg.n_layers)]\n",
    "mlp_norms = [cache[\"mlp_out\", l][0, -1].norm().item() for l in range(model.cfg.n_layers)]\n",
    "\n",
    "x = np.arange(model.cfg.n_layers)\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(x - width/2, attn_norms, width, label='Attention', alpha=0.8)\n",
    "plt.bar(x + width/2, mlp_norms, width, label='MLP', alpha=0.8)\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Output Norm')\n",
    "plt.title('Attention vs MLP Contribution by Layer')\n",
    "plt.legend()\n",
    "plt.xticks(x)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise 1: Different prompts\n",
    "Try different prompts and observe how attention patterns change.\n",
    "\n",
    "### Exercise 2: Find the \"previous token\" head\n",
    "Some heads learn to attend to the previous token. Can you find one?\n",
    "\n",
    "### Exercise 3: Track a specific token\n",
    "How does the representation of \"France\" change through the layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n",
    "# Try: \"Once upon a time, there was a\"\n",
    "# What does the model predict? What does attention look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've now:\n",
    "1. Loaded GPT-2 and run a forward pass\n",
    "2. Visualized attention patterns\n",
    "3. Explored how the residual stream evolves through layers\n",
    "4. Compared attention vs MLP contributions\n",
    "\n",
    "**Next:** [Chapter 3: The Residual Stream](https://ttsugriy.github.io/mechinterp-first-principles/chapters/03-residual-stream.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
